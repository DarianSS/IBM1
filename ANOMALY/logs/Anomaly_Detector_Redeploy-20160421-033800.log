Deploy script started at Thu Apr 21 03:38:00 EEST 2016
Running /users/Darian/PredictionIO-0.9.6//bin/pio train --verbose -v engine.json -- --executor-memory 16G --driver-memory 8G --total-executor-cores 4
[INFO] [Console$] Using existing engine manifest JSON at /Users/darian/PredictionIO-0.9.6/templates/ANOMALY/manifest.json
[INFO] [plugins] [Nicholas Scratch] loaded [], sites []
[INFO] [Runner$] Submission command: /users/Darian/PredictionIO-0.9.6/vendors/spark-1.5.1/bin/spark-submit --executor-memory 16G --driver-memory 8G --total-executor-cores 4 --class io.prediction.workflow.CreateWorkflow --jars file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template-assembly-0.1-SNAPSHOT-deps.jar,file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template_2.10-0.1-SNAPSHOT.jar --files file:/users/Darian/PredictionIO-0.9.6/conf/log4j.properties,file:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml --driver-class-path /users/Darian/PredictionIO-0.9.6/conf:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf file:/users/Darian/PredictionIO-0.9.6/lib/pio-assembly-0.9.6.jar --engine-id ulP05G3N7mAwusAYQ5cN1hrLFaDS1xwp --engine-version d3b3dc43fd7f00c3c7d9ac56b2678b6ad6396624 --engine-variant file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/engine.json --verbosity 0 --verbose --json-extractor Both --env PIO_STORAGE_SOURCES_HBASE_TYPE=hbase,PIO_ENV_LOADED=1,PIO_STORAGE_REPOSITORIES_METADATA_NAME=pio_meta,PIO_FS_BASEDIR=/Users/darian/.pio_store,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS=localhost,PIO_STORAGE_SOURCES_HBASE_HOME=/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0,PIO_HOME=/users/Darian/PredictionIO-0.9.6,PIO_FS_ENGINESDIR=/Users/darian/.pio_store/engines,PIO_STORAGE_SOURCES_LOCALFS_PATH=/Users/darian/.pio_store/models,PIO_STORAGE_SOURCES_PGSQL_URL=jdbc:postgresql://qdjjtnkv.db.elephantsql.com:5432/sfaupawb,PIO_STORAGE_SOURCES_HDFS_TYPE=hdfs,PIO_STORAGE_SOURCES_HDFS_PATH=/Users/darian/.pio_store/models,PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE=elasticsearch,PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH,PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS,PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event,PIO_STORAGE_SOURCES_PGSQL_PASSWORD=9-pVv4cOcih6kuZMmbDUZU32Qg-U6-eO,PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME=/users/Darian/PredictionIO-0.9.6/vendors/elasticsearch-1.4.4,PIO_STORAGE_SOURCES_PGSQL_TYPE=jdbc,PIO_FS_TMPDIR=/Users/darian/.pio_store/tmp,PIO_STORAGE_SOURCES_PGSQL_USERNAME=sfaupawb,PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model,PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE,PIO_CONF_DIR=/users/Darian/PredictionIO-0.9.6/conf,PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS=9300,PIO_STORAGE_SOURCES_LOCALFS_TYPE=localfs --verbose
Warning: Unknown option --verbose
[DEBUG] [] address: Darians-MacBook.local/192.168.99.1 isLoopbackAddress: false, with host 192.168.99.1 Darians-MacBook.local
[INFO] [Engine] Extracting datasource params...
[INFO] [WorkflowUtils$] No 'name' is found. Default empty String will be used.
[INFO] [Engine] Datasource params: (,ucl.team10.anomaly.DataSourceParams@3a4e343)
[INFO] [Engine] Extracting preparator params...
[INFO] [Engine] Preparator params: (,Empty)
[INFO] [Engine] Extracting serving params...
[INFO] [Engine] Serving params: (,Empty)
[INFO] [plugins] [Awesome Android] loaded [], sites []
[DEBUG] [CoreWorkflow$] Starting SparkContext
[DEBUG] [WorkflowContext$] Executor environment received: Map(PIO_STORAGE_SOURCES_HBASE_TYPE -> hbase, PIO_ENV_LOADED -> 1, PIO_STORAGE_REPOSITORIES_METADATA_NAME -> pio_meta, PIO_FS_BASEDIR -> /Users/darian/.pio_store, PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS -> localhost, PIO_STORAGE_SOURCES_HBASE_HOME -> /users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0, PIO_HOME -> /users/Darian/PredictionIO-0.9.6, PIO_FS_ENGINESDIR -> /Users/darian/.pio_store/engines, PIO_STORAGE_SOURCES_LOCALFS_PATH -> /Users/darian/.pio_store/models, PIO_STORAGE_SOURCES_PGSQL_URL -> jdbc:postgresql://qdjjtnkv.db.elephantsql.com:5432/sfaupawb, PIO_STORAGE_SOURCES_HDFS_TYPE -> hdfs, PIO_STORAGE_SOURCES_HDFS_PATH -> /Users/darian/.pio_store/models, PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE -> elasticsearch, PIO_STORAGE_REPOSITORIES_METADATA_SOURCE -> ELASTICSEARCH, PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE -> LOCALFS, PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME -> pio_event, PIO_STORAGE_SOURCES_PGSQL_PASSWORD -> 9-pVv4cOcih6kuZMmbDUZU32Qg-U6-eO, PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME -> /users/Darian/PredictionIO-0.9.6/vendors/elasticsearch-1.4.4, PIO_STORAGE_SOURCES_PGSQL_TYPE -> jdbc, PIO_FS_TMPDIR -> /Users/darian/.pio_store/tmp, PIO_STORAGE_SOURCES_PGSQL_USERNAME -> sfaupawb, PIO_STORAGE_REPOSITORIES_MODELDATA_NAME -> pio_model, PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE -> HBASE, PIO_CONF_DIR -> /users/Darian/PredictionIO-0.9.6/conf, PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS -> 9300, PIO_STORAGE_SOURCES_LOCALFS_TYPE -> localfs)
[DEBUG] [WorkflowContext$] SparkConf executor environment: ArraySeq((PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE,LOCALFS), (PIO_STORAGE_REPOSITORIES_METADATA_NAME,pio_meta), (PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME,pio_event), (PIO_STORAGE_SOURCES_HBASE_TYPE,hbase), (PIO_STORAGE_SOURCES_PGSQL_TYPE,jdbc), (PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE,HBASE), (PIO_STORAGE_SOURCES_HBASE_HOME,/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0), (PIO_ENV_LOADED,1), (PIO_FS_TMPDIR,/Users/darian/.pio_store/tmp), (PIO_STORAGE_SOURCES_PGSQL_PASSWORD,9-pVv4cOcih6kuZMmbDUZU32Qg-U6-eO), (PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME,/users/Darian/PredictionIO-0.9.6/vendors/elasticsearch-1.4.4), (PIO_FS_ENGINESDIR,/Users/darian/.pio_store/engines), (PIO_STORAGE_SOURCES_LOCALFS_TYPE,localfs), (PIO_CONF_DIR,/users/Darian/PredictionIO-0.9.6/conf), (PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE,elasticsearch), (PIO_STORAGE_SOURCES_PGSQL_USERNAME,sfaupawb), (PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS,9300), (PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS,localhost), (PIO_STORAGE_REPOSITORIES_MODELDATA_NAME,pio_model), (PIO_STORAGE_SOURCES_PGSQL_URL,jdbc:postgresql://qdjjtnkv.db.elephantsql.com:5432/sfaupawb), (PIO_STORAGE_SOURCES_HDFS_TYPE,hdfs), (PIO_FS_BASEDIR,/Users/darian/.pio_store), (PIO_STORAGE_SOURCES_HDFS_PATH,/Users/darian/.pio_store/models), (PIO_STORAGE_REPOSITORIES_METADATA_SOURCE,ELASTICSEARCH), (PIO_STORAGE_SOURCES_LOCALFS_PATH,/Users/darian/.pio_store/models), (PIO_HOME,/users/Darian/PredictionIO-0.9.6))
[DEBUG] [WorkflowContext$] Application environment received: Map(spark.executor.extraClassPath -> .)
[DEBUG] [WorkflowContext$] SparkConf environment: WrappedArray((spark.files,file:/users/Darian/PredictionIO-0.9.6/conf/log4j.properties,file:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE,LOCALFS), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_METADATA_NAME,pio_meta), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME,pio_event), (spark.executorEnv.PIO_STORAGE_SOURCES_HBASE_TYPE,hbase), (spark.executor.extraClassPath,.), (spark.executorEnv.PIO_STORAGE_SOURCES_PGSQL_TYPE,jdbc), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE,HBASE), (spark.executorEnv.PIO_STORAGE_SOURCES_HBASE_HOME,/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0), (spark.executorEnv.PIO_ENV_LOADED,1), (spark.app.name,PredictionIO Training: ucl.team10.anomaly.AnomalyEngine), (spark.master,local[*]), (spark.executorEnv.PIO_FS_TMPDIR,/Users/darian/.pio_store/tmp), (spark.executorEnv.PIO_STORAGE_SOURCES_PGSQL_PASSWORD,9-pVv4cOcih6kuZMmbDUZU32Qg-U6-eO), (spark.submit.deployMode,client), (spark.executorEnv.PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME,/users/Darian/PredictionIO-0.9.6/vendors/elasticsearch-1.4.4), (spark.executorEnv.PIO_FS_ENGINESDIR,/Users/darian/.pio_store/engines), (spark.executorEnv.PIO_STORAGE_SOURCES_LOCALFS_TYPE,localfs), (spark.executorEnv.PIO_CONF_DIR,/users/Darian/PredictionIO-0.9.6/conf), (spark.executorEnv.PIO_STORAGE_SOURCES_ELASTICSEARCH_TYPE,elasticsearch), (spark.jars,file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template-assembly-0.1-SNAPSHOT-deps.jar,file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template_2.10-0.1-SNAPSHOT.jar,file:/users/Darian/PredictionIO-0.9.6/lib/pio-assembly-0.9.6.jar), (spark.driver.extraClassPath,/users/Darian/PredictionIO-0.9.6/conf:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf), (spark.executorEnv.PIO_STORAGE_SOURCES_PGSQL_USERNAME,sfaupawb), (spark.driver.memory,8G), (spark.executorEnv.PIO_STORAGE_SOURCES_ELASTICSEARCH_PORTS,9300), (spark.executorEnv.PIO_STORAGE_SOURCES_ELASTICSEARCH_HOSTS,localhost), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_MODELDATA_NAME,pio_model), (spark.executorEnv.PIO_STORAGE_SOURCES_PGSQL_URL,jdbc:postgresql://qdjjtnkv.db.elephantsql.com:5432/sfaupawb), (spark.executorEnv.PIO_STORAGE_SOURCES_HDFS_TYPE,hdfs), (spark.executorEnv.PIO_FS_BASEDIR,/Users/darian/.pio_store), (spark.executorEnv.PIO_STORAGE_SOURCES_HDFS_PATH,/Users/darian/.pio_store/models), (spark.executorEnv.PIO_STORAGE_REPOSITORIES_METADATA_SOURCE,ELASTICSEARCH), (spark.executorEnv.PIO_STORAGE_SOURCES_LOCALFS_PATH,/Users/darian/.pio_store/models), (spark.executorEnv.PIO_HOME,/users/Darian/PredictionIO-0.9.6))
[INFO] [SparkContext] Running Spark version 1.5.1
[INFO] [SecurityManager] Changing view acls to: darian
[INFO] [SecurityManager] Changing modify acls to: darian
[INFO] [SecurityManager] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(darian); users with modify permissions: Set(darian)
[INFO] [Slf4jLogger] Slf4jLogger started
[INFO] [Remoting] Starting remoting
[INFO] [Remoting] Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.99.1:65399]
[INFO] [Utils] Successfully started service 'sparkDriver' on port 65399.
[INFO] [SparkEnv] Registering MapOutputTracker
[INFO] [SparkEnv] Registering BlockManagerMaster
[INFO] [DiskBlockManager] Created local directory at /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/blockmgr-da9653ec-3a23-48bd-aa20-9aada7ba931a
[INFO] [MemoryStore] MemoryStore started with capacity 4.1 GB
[INFO] [HttpFileServer] HTTP File server directory is /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/httpd-1be4587f-8179-482e-bfa1-326aed5fe27f
[INFO] [HttpServer] Starting HTTP Server
[INFO] [Utils] Successfully started service 'HTTP file server' on port 65400.
[INFO] [SparkEnv] Registering OutputCommitCoordinator
[INFO] [Utils] Successfully started service 'SparkUI' on port 4040.
[INFO] [SparkUI] Started SparkUI at http://192.168.99.1:4040
[INFO] [SparkContext] Added JAR file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template-assembly-0.1-SNAPSHOT-deps.jar at http://192.168.99.1:65400/jars/barebone-template-assembly-0.1-SNAPSHOT-deps.jar with timestamp 1461199088218
[INFO] [SparkContext] Added JAR file:/Users/darian/PredictionIO-0.9.6/templates/ANOMALY/target/scala-2.10/barebone-template_2.10-0.1-SNAPSHOT.jar at http://192.168.99.1:65400/jars/barebone-template_2.10-0.1-SNAPSHOT.jar with timestamp 1461199088228
[INFO] [SparkContext] Added JAR file:/users/Darian/PredictionIO-0.9.6/lib/pio-assembly-0.9.6.jar at http://192.168.99.1:65400/jars/pio-assembly-0.9.6.jar with timestamp 1461199088917
[INFO] [Utils] Copying /users/Darian/PredictionIO-0.9.6/conf/log4j.properties to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/log4j.properties
[INFO] [SparkContext] Added file file:/users/Darian/PredictionIO-0.9.6/conf/log4j.properties at file:/users/Darian/PredictionIO-0.9.6/conf/log4j.properties with timestamp 1461199088927
[INFO] [Utils] Copying /users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/hbase-site.xml
[INFO] [SparkContext] Added file file:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml at file:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml with timestamp 1461199088948
[WARN] [MetricsSystem] Using default name DAGScheduler for source because spark.app.id is not set.
[INFO] [Executor] Starting executor ID driver on host localhost
[DEBUG] [InternalLoggerFactory] Using SLF4J as the default logging framework
[DEBUG] [PlatformDependent0] java.nio.Buffer.address: available
[DEBUG] [PlatformDependent0] sun.misc.Unsafe.theUnsafe: available
[DEBUG] [PlatformDependent0] sun.misc.Unsafe.copyMemory: available
[DEBUG] [PlatformDependent0] java.nio.Bits.unaligned: true
[DEBUG] [PlatformDependent] Java version: 8
[DEBUG] [PlatformDependent] -Dio.netty.noUnsafe: false
[DEBUG] [PlatformDependent] sun.misc.Unsafe: available
[DEBUG] [PlatformDependent] -Dio.netty.noJavassist: false
[DEBUG] [PlatformDependent] Javassist: unavailable
[DEBUG] [PlatformDependent] You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.
[DEBUG] [PlatformDependent] -Dio.netty.tmpdir: /var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T (java.io.tmpdir)
[DEBUG] [PlatformDependent] -Dio.netty.bitMode: 64 (sun.arch.data.model)
[DEBUG] [PlatformDependent] -Dio.netty.noPreferDirect: false
[DEBUG] [MultithreadEventLoopGroup] -Dio.netty.eventLoopThreads: 16
[DEBUG] [NioEventLoop] -Dio.netty.noKeySetOptimization: false
[DEBUG] [NioEventLoop] -Dio.netty.selectorAutoRebuildThreshold: 512
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@773c2214
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@15e1f8fe
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@110b7837
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@6ee88e21
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@78d23d6a
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@626e0c86
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@28ee0a3c
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@2dd1086
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.numHeapArenas: 16
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.numDirectArenas: 16
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.pageSize: 8192
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.maxOrder: 11
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.chunkSize: 16777216
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.tinyCacheSize: 512
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.smallCacheSize: 256
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.normalCacheSize: 64
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.maxCachedBufferCapacity: 32768
[DEBUG] [PooledByteBufAllocator] -Dio.netty.allocator.cacheTrimInterval: 8192
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@62d40e31
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@650aa077
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@7ce29a2d
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@457a5b2d
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@79d7035
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@372461a9
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@5fffb692
[TRACE] [NioEventLoop] Instrumented an optimized java.util.Set into: sun.nio.ch.KQueueSelectorImpl@48cb2d73
[DEBUG] [ThreadLocalRandom] -Dio.netty.initialSeedUniquifier: 0x3a13ddf021a19b86 (took 0 ms)
[DEBUG] [ByteBufUtil] -Dio.netty.allocator.type: unpooled
[DEBUG] [ByteBufUtil] -Dio.netty.threadLocalDirectBufferSize: 65536
[DEBUG] [NetUtil] Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1)
[DEBUG] [NetUtil] /proc/sys/net/core/somaxconn: 128 (non-existent)
[INFO] [Utils] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65401.
[INFO] [NettyBlockTransferService] Server created on 65401
[INFO] [BlockManagerMaster] Trying to register BlockManager
[INFO] [BlockManagerMasterEndpoint] Registering block manager localhost:65401 with 4.1 GB RAM, BlockManagerId(driver, localhost, 65401)
[INFO] [BlockManagerMaster] Registered BlockManager
[INFO] [Engine$] EngineWorkflow.train
[INFO] [Engine$] DataSource: ucl.team10.anomaly.DataSource@21e5f0b6
[INFO] [Engine$] Preparator: ucl.team10.anomaly.Preparator@656842bc
[INFO] [Engine$] AlgorithmList: List(ucl.team10.anomaly.Algorithm@7a606260)
[INFO] [Engine$] Data sanity check is on.
[INFO] [Engine$] org.apache.spark.rdd.MapPartitionsRDD does not support data sanity check. Skipping check.
[INFO] [Engine$] org.apache.spark.rdd.MapPartitionsRDD does not support data sanity check. Skipping check.
[INFO] [Engine$] org.apache.spark.rdd.MapPartitionsRDD does not support data sanity check. Skipping check.
[INFO] [Engine$] EngineWorkflow.train completed
[INFO] [Engine] engineInstanceId=AVQ2QKKMDx2S-HeeYplI
[INFO] [SparkContext] Starting job: first at LAlgorithm.scala:118
[INFO] [DAGScheduler] Got job 0 (first at LAlgorithm.scala:118) with 1 output partitions
[INFO] [DAGScheduler] Final stage: ResultStage 0(first at LAlgorithm.scala:118)
[INFO] [DAGScheduler] Parents of final stage: List()
[INFO] [DAGScheduler] Missing parents: List()
[INFO] [DAGScheduler] Submitting ResultStage 0 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45), which has no missing parents
[INFO] [MemoryStore] ensureFreeSpace(3800) called with curMem=0, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_0 stored as values in memory (estimated size 3.7 KB, free 4.1 GB)
[INFO] [MemoryStore] ensureFreeSpace(2132) called with curMem=3800, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
[INFO] [BlockManagerInfo] Added broadcast_0_piece0 in memory on localhost:65401 (size: 2.1 KB, free: 4.1 GB)
[INFO] [SparkContext] Created broadcast 0 from broadcast at DAGScheduler.scala:861
[INFO] [DAGScheduler] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45)
[INFO] [TaskSchedulerImpl] Adding task set 0.0 with 1 tasks
[INFO] [TaskSetManager] Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [Executor] Running task 0.0 in stage 0.0 (TID 0)
[INFO] [Executor] Fetching file:/users/Darian/PredictionIO-0.9.6/conf/log4j.properties with timestamp 1461199088927
[INFO] [Utils] /users/Darian/PredictionIO-0.9.6/conf/log4j.properties has been previously copied to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/log4j.properties
[INFO] [Executor] Fetching file:/users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml with timestamp 1461199088948
[INFO] [Utils] /users/Darian/PredictionIO-0.9.6/vendors/hbase-1.0.0/conf/hbase-site.xml has been previously copied to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/hbase-site.xml
[INFO] [Executor] Fetching http://192.168.99.1:65400/jars/barebone-template_2.10-0.1-SNAPSHOT.jar with timestamp 1461199088228
[INFO] [Utils] Fetching http://192.168.99.1:65400/jars/barebone-template_2.10-0.1-SNAPSHOT.jar to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/fetchFileTemp9030950839269797778.tmp
[INFO] [Executor] Adding file:/private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/barebone-template_2.10-0.1-SNAPSHOT.jar to class loader
[INFO] [Executor] Fetching http://192.168.99.1:65400/jars/barebone-template-assembly-0.1-SNAPSHOT-deps.jar with timestamp 1461199088218
[INFO] [Utils] Fetching http://192.168.99.1:65400/jars/barebone-template-assembly-0.1-SNAPSHOT-deps.jar to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/fetchFileTemp530489221696256559.tmp
[INFO] [Executor] Adding file:/private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/barebone-template-assembly-0.1-SNAPSHOT-deps.jar to class loader
[INFO] [Executor] Fetching http://192.168.99.1:65400/jars/pio-assembly-0.9.6.jar with timestamp 1461199088917
[INFO] [Utils] Fetching http://192.168.99.1:65400/jars/pio-assembly-0.9.6.jar to /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/fetchFileTemp4123942207360127198.tmp
[INFO] [Executor] Adding file:/private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948/userFiles-83d0bbcb-f452-432d-a5b8-d46e0393aa3b/pio-assembly-0.9.6.jar to class loader
[INFO] [Executor] Finished task 0.0 in stage 0.0 (TID 0). 915 bytes result sent to driver
[INFO] [TaskSetManager] Finished task 0.0 in stage 0.0 (TID 0) in 1087 ms on localhost (1/1)
[INFO] [TaskSchedulerImpl] Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] [DAGScheduler] ResultStage 0 (first at LAlgorithm.scala:118) finished in 1,106 s
[INFO] [DAGScheduler] Job 0 finished: first at LAlgorithm.scala:118, took 1,354577 s
[INFO] [SparkContext] Starting job: first at LAlgorithm.scala:118
[INFO] [DAGScheduler] Got job 1 (first at LAlgorithm.scala:118) with 4 output partitions
[INFO] [DAGScheduler] Final stage: ResultStage 1(first at LAlgorithm.scala:118)
[INFO] [DAGScheduler] Parents of final stage: List()
[INFO] [DAGScheduler] Missing parents: List()
[INFO] [DAGScheduler] Submitting ResultStage 1 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45), which has no missing parents
[INFO] [MemoryStore] ensureFreeSpace(3800) called with curMem=5932, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 4.1 GB)
[INFO] [MemoryStore] ensureFreeSpace(2132) called with curMem=9732, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
[INFO] [BlockManagerInfo] Added broadcast_1_piece0 in memory on localhost:65401 (size: 2.1 KB, free: 4.1 GB)
[INFO] [SparkContext] Created broadcast 1 from broadcast at DAGScheduler.scala:861
[INFO] [DAGScheduler] Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45)
[INFO] [TaskSchedulerImpl] Adding task set 1.0 with 4 tasks
[INFO] [TaskSetManager] Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [TaskSetManager] Starting task 1.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [TaskSetManager] Starting task 2.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [TaskSetManager] Starting task 3.0 in stage 1.0 (TID 4, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [Executor] Running task 0.0 in stage 1.0 (TID 1)
[INFO] [Executor] Running task 1.0 in stage 1.0 (TID 2)
[INFO] [Executor] Running task 2.0 in stage 1.0 (TID 3)
[INFO] [Executor] Running task 3.0 in stage 1.0 (TID 4)
[INFO] [Executor] Finished task 3.0 in stage 1.0 (TID 4). 915 bytes result sent to driver
[INFO] [Executor] Finished task 0.0 in stage 1.0 (TID 1). 915 bytes result sent to driver
[INFO] [Executor] Finished task 1.0 in stage 1.0 (TID 2). 915 bytes result sent to driver
[INFO] [TaskSetManager] Finished task 3.0 in stage 1.0 (TID 4) in 7 ms on localhost (1/4)
[INFO] [Executor] Finished task 2.0 in stage 1.0 (TID 3). 915 bytes result sent to driver
[INFO] [TaskSetManager] Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (2/4)
[INFO] [TaskSetManager] Finished task 2.0 in stage 1.0 (TID 3) in 9 ms on localhost (3/4)
[INFO] [TaskSetManager] Finished task 1.0 in stage 1.0 (TID 2) in 10 ms on localhost (4/4)
[INFO] [TaskSchedulerImpl] Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] [DAGScheduler] ResultStage 1 (first at LAlgorithm.scala:118) finished in 0,011 s
[INFO] [DAGScheduler] Job 1 finished: first at LAlgorithm.scala:118, took 0,021414 s
[INFO] [SparkContext] Starting job: first at LAlgorithm.scala:118
[INFO] [DAGScheduler] Got job 2 (first at LAlgorithm.scala:118) with 3 output partitions
[INFO] [DAGScheduler] Final stage: ResultStage 2(first at LAlgorithm.scala:118)
[INFO] [DAGScheduler] Parents of final stage: List()
[INFO] [DAGScheduler] Missing parents: List()
[INFO] [DAGScheduler] Submitting ResultStage 2 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45), which has no missing parents
[INFO] [MemoryStore] ensureFreeSpace(3800) called with curMem=11864, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 4.1 GB)
[INFO] [MemoryStore] ensureFreeSpace(2132) called with curMem=15664, maxMem=4445479895
[INFO] [MemoryStore] Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 4.1 GB)
[INFO] [BlockManagerInfo] Added broadcast_2_piece0 in memory on localhost:65401 (size: 2.1 KB, free: 4.1 GB)
[INFO] [SparkContext] Created broadcast 2 from broadcast at DAGScheduler.scala:861
[INFO] [DAGScheduler] Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at map at LAlgorithm.scala:45)
[INFO] [TaskSchedulerImpl] Adding task set 2.0 with 3 tasks
[INFO] [TaskSetManager] Starting task 0.0 in stage 2.0 (TID 5, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [TaskSetManager] Starting task 1.0 in stage 2.0 (TID 6, localhost, PROCESS_LOCAL, 2470 bytes)
[INFO] [TaskSetManager] Starting task 2.0 in stage 2.0 (TID 7, localhost, PROCESS_LOCAL, 2475 bytes)
[INFO] [Executor] Running task 2.0 in stage 2.0 (TID 7)
[INFO] [Executor] Running task 0.0 in stage 2.0 (TID 5)
[INFO] [Executor] Running task 1.0 in stage 2.0 (TID 6)
[INFO] [Executor] Finished task 1.0 in stage 2.0 (TID 6). 915 bytes result sent to driver
[INFO] [Executor] Finished task 0.0 in stage 2.0 (TID 5). 915 bytes result sent to driver
[INFO] [TaskSetManager] Finished task 1.0 in stage 2.0 (TID 6) in 7 ms on localhost (1/3)
[INFO] [TaskSetManager] Finished task 0.0 in stage 2.0 (TID 5) in 8 ms on localhost (2/3)
[INFO] [HConnectionManager$HConnectionImplementation] Closing master protocol: MasterService
[INFO] [HConnectionManager$HConnectionImplementation] Closing zookeeper sessionid=0x15435ffa08d000b
[WARN] [HConnectionManager$HConnectionImplementation] Encountered problems when prefetch hbase:meta table: 
[WARN] [HConnectionManager$HConnectionImplementation] Encountered problems when prefetch hbase:meta table: 
[WARN] [ClientScanner] scanner failed to close. Exception follows: org.apache.hadoop.hbase.TableNotFoundException: pio_event:events_4
[ERROR] [Executor] Exception in task 2.0 in stage 2.0 (TID 7)
[WARN] [ThrowableSerializationWrapper] Task exception could not be deserialized
[ERROR] [TaskResultGetter] Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@2bbf4b8b
[WARN] [TaskSetManager] Lost task 2.0 in stage 2.0 (TID 7, localhost): UnknownReason
[ERROR] [TaskSetManager] Task 2 in stage 2.0 failed 1 times; aborting job
[INFO] [TaskSchedulerImpl] Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] [TaskSchedulerImpl] Cancelling stage 2
[INFO] [DAGScheduler] ResultStage 2 (first at LAlgorithm.scala:118) failed in 0,829 s
[INFO] [DAGScheduler] Job 2 failed: first at LAlgorithm.scala:118, took 0,840725 s
[DEBUG] [CoreWorkflow$] Stopping SparkContext
[INFO] [SparkUI] Stopped Spark web UI at http://192.168.99.1:4040
[INFO] [DAGScheduler] Stopping DAGScheduler
[INFO] [MapOutputTrackerMasterEndpoint] MapOutputTrackerMasterEndpoint stopped!
[INFO] [MemoryStore] MemoryStore cleared
[INFO] [BlockManager] BlockManager stopped
[INFO] [BlockManagerMaster] BlockManagerMaster stopped
[INFO] [OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] OutputCommitCoordinator stopped!
[INFO] [SparkContext] Successfully stopped SparkContext
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 7, localhost): UnknownReason
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1298)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1272)
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1312)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1311)
	at io.prediction.controller.LAlgorithm.makePersistentModel(LAlgorithm.scala:118)
	at io.prediction.controller.Engine$$anonfun$makeSerializableModels$2.apply(Engine.scala:295)
	at io.prediction.controller.Engine$$anonfun$makeSerializableModels$2.apply(Engine.scala:294)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at io.prediction.controller.Engine.makeSerializableModels(Engine.scala:294)
	at io.prediction.controller.Engine.train(Engine.scala:185)
	at io.prediction.workflow.CoreWorkflow$.runTrain(CoreWorkflow.scala:65)
	at io.prediction.workflow.CreateWorkflow$.main(CreateWorkflow.scala:247)
	at io.prediction.workflow.CreateWorkflow.main(CreateWorkflow.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[INFO] [ShutdownHookManager] Shutdown hook called
[INFO] [ShutdownHookManager] Deleting directory /private/var/folders/q8/2xqj5rx92x9gs2f39tr6jq8r0000gn/T/spark-5c9cc701-30dd-46fe-8748-b00d20803948
Training ended with return value 1 at Thu Apr 21 03:38:13 EEST 2016
